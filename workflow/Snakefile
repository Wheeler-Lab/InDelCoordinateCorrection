configfile: "config/config.yaml"

from utils import ExperimentTree

# Digest the config and produce the dependency tree of all experiments
tree = ExperimentTree(config)

# Default rule, requests the GFF files for all experiments in the tree
rule all:
    input: tree.all_inputs

# Download any genomes from TrypTag - this is relevant mostly for the reference genomes
rule download_tryptrypdb_fasta:
    output:
        fasta="{path}/TriTrypDB-{version}_{organism}_Genome.fasta",
    shell: """
        curl -o {output.fasta} "https://tritrypdb.org/common/downloads/release-{wildcards.version}/{wildcards.organism}/fasta/data/TriTrypDB-{wildcards.version}_{wildcards.organism}_Genome.fasta"
    """

# Download sequencing samples from the SRA using sra-tools (any sample name starting with "SRR")
rule download_sra:
    output:
        r1=temp("{path}/SRR{sraid}_1.fq"),
        r2=temp("{path}/SRR{sraid}_2.fq"),
        store=temp(directory("{path}/SRR{sraid}")),
    wildcard_constraints:
        sraid="[0-9]+",
    conda: "envs/sratools.yaml"
    shell: """
        cd {wildcards.path}
        # Prefetching followed by fasterq-dump is faster than fasterq-dump alone (not sure why?)
        prefetch SRR{wildcards.sraid}
        fasterq-dump --split-files SRR{wildcards.sraid}
        # Move to correct location
        mv SRR{wildcards.sraid}_1.fastq SRR{wildcards.sraid}_1.fq
        mv SRR{wildcards.sraid}_2.fastq SRR{wildcards.sraid}_2.fq
    """

# Uncompresses gzipped fastq files
rule uncompress:
    input: lambda wildcards: tree.samples_directory / wildcards.experiment / f"{wildcards.sample_id}_{wildcards.runid}.fq.gz"
    output: temp("results/{experiment}/{sample_id}_{runid}.fq")
    wildcard_constraints:
        runid="[12]",
    resources:
        uncompress=1
    shell: """
    gunzip < "{input}" > "{output}"
    """

# Run rcorrector on the sequencing samples to correct spurious k-mers
rule rcorrector:
    input:
        r1="results/{experiment}/{sample_id}_1.fq",
        r2="results/{experiment}/{sample_id}_2.fq",
    output:
        r1=temp("results/{experiment}/{sample_id}_1.cor.fq"),
        r2=temp("results/{experiment}/{sample_id}_2.cor.fq"),
    conda: "envs/rcorrector.yaml"
    threads: 24
    shell: """
    run_rcorrector.pl -t 24 -1 {input.r1} -2 {input.r2} -od results/{wildcards.experiment}/
    """

# Filter out reads that rcorrector has marked as uncorrectable
rule filter_rcorrector_reads:
    input:
        r1="results/{experiment}/{sample_id}_1.cor.fq",
        r2="results/{experiment}/{sample_id}_2.cor.fq",
    output:
        r1=temp("results/{experiment}/{sample_id}_1.filtered.fq"),
        r2=temp("results/{experiment}/{sample_id}_2.filtered.fq"),
    threads: 1
    run:
        from itertools import zip_longest
        def grouper(iterable, n, fillvalue=None):
            "Collect data into fixed-length chunks or blocks"
            # grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx
            args = [iter(iterable)] * n
            return zip_longest(fillvalue=fillvalue, *args)

        r1_cor_count=0
        r2_cor_count=0
        pair_cor_count=0
        unfix_r1_count=0
        unfix_r2_count=0
        unfix_both_count=0   

        with \
            open(input.r1, 'r') as r1_in,\
            open(input.r2, 'r') as r2_in,\
            open(output.r1, 'w') as r1_out,\
            open(output.r2, 'w') as r2_out:
            R1=grouper(r1_in,4)
            R2=grouper(r2_in,4)
            counter=0
            for entry in R1:
                counter+=1
                if counter%100000==0:
                    print("%s reads processed" % counter)
            
                head1,seq1,placeholder1,qual1=[i.strip() for i in entry]
                head2,seq2,placeholder2,qual2=[j.strip() for j in next(R2)]
                
                if 'unfixable' in head1 and 'unfixable' not in head2:
                    unfix_r1_count+=1
                elif 'unfixable' in head2 and 'unfixable' not in head1:
                    unfix_r2_count+=1
                elif 'unfixable' in head1 and 'unfixable' in head2:
                    unfix_both_count+=1
                else:
                    if 'cor' in head1:
                        r1_cor_count+=1
                    if 'cor' in head2:
                        r2_cor_count+=1
                    if 'cor' in head1 or 'cor' in head2:
                        pair_cor_count+=1
                    
                    r1_out.write('%s\n' % '\n'.join([head1,seq1,placeholder1,qual1]))
                    r2_out.write('%s\n' % '\n'.join([head2,seq2,placeholder2,qual2]))
        
        total_unfixable = unfix_r1_count+unfix_r2_count+unfix_both_count
        total_retained = counter - total_unfixable

        print('total PE reads:%s\nremoved PE reads:%s\nretained PE reads:%s\nR1 corrected:%s\nR2 corrected:%s\npairs corrected:%s\nR1 unfixable:%s\nR2 unfixable:%s\nboth reads unfixable:%s\n' % (counter,total_unfixable,total_retained,r1_cor_count,r2_cor_count,pair_cor_count,unfix_r1_count,unfix_r2_count,unfix_both_count))

# Run Trim Galore to trim adapter sequences
rule trim_galore:
    input:
        r1="results/{experiment}/{sample_id}_1.filtered.fq",
        r2="results/{experiment}/{sample_id}_2.filtered.fq",
    output:
        r1=temp("results/{experiment}/{sample_id}_val_1.fq"),
        r2=temp("results/{experiment}/{sample_id}_val_2.fq"),
    conda: "envs/trimgalore.yaml"
    threads: 8
    shell: """
    trim_galore --paired --phred33 --length 36 -q 5 --stringency 1 -e 0.1 -j 8 --no_report_file --dont_gzip --output_dir 'results/{wildcards.experiment}' --basename '{wildcards.sample_id}' '{input.r1}' '{input.r2}'
    """

# Index a fasta file containing a genome sequence (to align the reads to below)
rule bwa_mem2_index:
    input:
        "{genome}",
    output:
        temp("{genome}.0123"),
        temp("{genome}.amb"),
        temp("{genome}.ann"),
        temp("{genome}.bwt.2bit.64"),
        temp("{genome}.pac"),
    log:
        "logs/bwa-mem2_index/{genome}.log",
    wrapper:
        "v1.32.0/bio/bwa-mem2/index"

# Align reads using BWA-MEM2
rule bwa_mem2_mem:
    input:
        reads=["{sample_path}_val_1.fq", "{sample_path}_val_2.fq"],
        # Index can be a list of (all) files created by bwa, or one of them
        idx=lambda wildcards: multiext(str(tree.fasta_lookup[wildcards.sample_path]), ".0123", ".amb", ".ann", ".bwt.2bit.64", ".pac"),
    output:
        temp("{sample_path}.bam"),
    log:
        "logs/bwa_mem2/{sample_path}.log",
    params:
        extra=r"-R '@RG\tID:{sample_path}\tSM:{sample_path}'",
        sort="samtools",  # Can be 'none', 'samtools' or 'picard'.
        sort_order="coordinate",  # Can be 'coordinate' (default) or 'queryname'.
        sort_extra="-m 4G",  # Extra args for samtools/picard.
    threads: 8
    wrapper:
        "v1.32.0/bio/bwa-mem2/mem"

# Produce an index for the aligned reads
rule samtools_index:
    input:
        "{sample}.bam",
    output:
        temp("{sample}.bam.bai"),
    log:
        "logs/samtools_index/{sample}.log",
    params:
        extra="",  # optional params string
    threads: 4  # This value - 1 will be sent to -@
    wrapper:
        "v1.32.0/bio/samtools/index"

# Run pilon to polish the parental genome and produce a corrected genome sequence with enumerated changes (allowing indels and snps)
rule pilon_polish:
    input:
        frags=lambda wildcards: expand(f'results/{wildcards.experiment}/{{sample}}.bam', sample=tree.dependencies[wildcards.experiment]['samples']),
        frags_index=lambda wildcards: expand(f'results/{wildcards.experiment}/{{sample}}.bam.bai', sample=tree.dependencies[wildcards.experiment]['samples']),
        genome=lambda wildcards: tree.dependencies[wildcards.experiment]['input_fasta'],
    output:
        fasta=temp("results/{experiment}/genome.polish.fasta"),
        changes=temp("results/{experiment}/genome.polish.changes"),
    threads: 8
    conda: "envs/pilon.yaml"
    shell: """
    pilon -Xmx20G --genome {input.genome} --output results/{wildcards.experiment}/genome.polish --changes --fix snps,indels `for frag in {input.frags}; do echo -n "--frags $frag "; done;`
    """

# Remove the annoying "_pilon" contig name suffix in the fasta file from the pilon run
rule fasta_remove_pilon_suffix:
    input: "results/{experiment}/genome.polish.fasta",
    output: "results/{experiment}/genome.fasta",
    shell: "sed 's/_pilon//' {input} > {output}"

# Shift genome features to their correct positions after the changes from the pilon run have been taken into account
rule coordinate_shifts:
    input:
        gff=lambda wildcards: tree.dependencies[wildcards.experiment]['input_gff'],
        changes="results/{experiment}/genome.polish.changes",
        fasta="results/{experiment}/genome.fasta",
    output:
        gff="results/{experiment}/genome.gff",
    conda: "envs/shift_gff_coordinates.yaml"
    script: "scripts/shift_gff_coordinates.py"